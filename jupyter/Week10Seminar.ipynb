{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "\n",
    "import lightfm\n",
    "import lightfm.data as ld\n",
    "import lightfm.evaluation as lv\n",
    "\n",
    "import tqdm\n",
    "import json\n",
    "import glob\n",
    "import faiss\n",
    "import typing\n",
    "#import optuna\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(31337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Этап 1. Обучение вспомогательной модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Возьмем данные с предыдущего семинара (можете сгенерить самостоятельно)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([\n",
    "    pd.read_json(data_path, lines=True)\n",
    "    for data_path\n",
    "    in glob.glob(\"./stage_1/train/*/*\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_sample = np.random.uniform(size=data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stage_1 = data[random_sample < 0.4]\n",
    "data_stage_2 = data[random_sample >= 0.4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "positives = data_stage_1[data_stage_1[\"time\"] > 0.7].copy()\n",
    "positives[\"test\"] = np.random.uniform(size=len(positives)) >= 0.95\n",
    "positives.drop_duplicates([\"user\", \"track\"], inplace=True)\n",
    "\n",
    "user_counts = positives[~positives[\"test\"]].groupby(\"user\").size()\n",
    "users = set(user_counts[user_counts >= 2].index.values)\n",
    "\n",
    "track_counts = positives[~positives[\"test\"]].groupby(\"track\").size()\n",
    "tracks = set(track_counts[track_counts >= 2].index.values)\n",
    "\n",
    "len(users), len(tracks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Снова обучим LightFM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном случае LightFM - это наша вспомогательная модель, которая поможет нам рассчитать IPS.\n",
    "\n",
    "Для обучения возьмем небольшой сабсет данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = positives[~positives[\"test\"] & positives[\"user\"].isin(users) & positives[\"track\"].isin(tracks)]\n",
    "test_data = positives[positives[\"test\"] & positives[\"user\"].isin(users) & positives[\"track\"].isin(tracks)]\n",
    "\n",
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = ld.Dataset()\n",
    "dataset.fit(users, tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_interactions, _ = dataset.build_interactions(train_data[[\"user\", \"track\"]].itertuples(index=False, name=None))\n",
    "test_interactions, _ = dataset.build_interactions(test_data[[\"user\", \"track\"]].itertuples(index=False, name=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fit_model(\n",
    "    epochs=1, \n",
    "    at=10,\n",
    "    loss=\"warp\",\n",
    "    no_components=30,\n",
    "    learning_rate=0.01, \n",
    "    max_sampled=10,\n",
    "    user_alpha=0.0, \n",
    "    item_alpha=0.0, \n",
    "    threads=30, \n",
    "    verbose=False,\n",
    "    patience=3,\n",
    "    epsilon=1e-6,\n",
    "):\n",
    "    model = lightfm.LightFM(\n",
    "        no_components=no_components,\n",
    "        loss=loss,\n",
    "        learning_rate=learning_rate,\n",
    "        max_sampled=max_sampled,\n",
    "        user_alpha=user_alpha,\n",
    "        item_alpha=item_alpha,\n",
    "    )\n",
    "\n",
    "    precisions_at = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model = model.fit_partial(train_interactions, num_threads=threads)\n",
    "        \n",
    "        precision_at = lv.precision_at_k(model, test_interactions, train_interactions=train_interactions, k=at, num_threads=threads)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"{epoch}:\\t{np.mean(precision_at)} +/- {ss.sem(precision_at) * 1.96}\")\n",
    "            \n",
    "        precisions_at.append(np.mean(precision_at))\n",
    "            \n",
    "        if epoch > patience and all([precisions_at[-j] - precisions_at[-patience-1] < epsilon for j in range(1, patience + 1)]):\n",
    "            if verbose:\n",
    "                print(\"Early stopiing!\")\n",
    "            break\n",
    "        \n",
    "    else:\n",
    "        if verbose:\n",
    "            print(\"No early stopiing happened: increase epochs maybe?\")\n",
    "        \n",
    "    return model, precisions_at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model, precisions_at = fit_model(\n",
    "    epochs=100,\n",
    "    at=10,\n",
    "    loss='warp',\n",
    "    no_components=50, \n",
    "    learning_rate=0.01,\n",
    "    max_sampled=100,\n",
    "    user_alpha=0.0,\n",
    "    item_alpha=0.0001,\n",
    "    patience=15,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, ax = plt.subplots()\n",
    "ax.plot(np.arange(len(precisions_at)), precisions_at)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сохраняем эмбеддинги и готовим рекомендации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "item_biases, item_embeddings = model.get_item_representations()\n",
    "user_biases, user_embeddings = model.get_user_representations()\n",
    "\n",
    "np.save(\"./stage_1/item_biases\", item_biases)\n",
    "np.save(\"./stage_1/item_embeddings\", item_embeddings)\n",
    "np.save(\"./stage_1/user_biases\", user_biases)\n",
    "np.save(\"./stage_1/user_embeddings\", user_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITEM_BIASES = np.load(\"./stage_1/item_biases.npy\")\n",
    "ITEM_EMBEDDINGS = np.load(\"./stage_1/item_embeddings.npy\")\n",
    "USER_BIASES = np.load(\"./stage_1/user_biases.npy\")\n",
    "USER_EMBEDDINGS = np.load(\"./stage_1/user_embeddings.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Делаем маппинги индекс трека -> айди трека"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRACK_META = pd.read_json(\"./data/tracks.json\", lines=True)\n",
    "TRACK_META[\"track_index\"] = TRACK_META[\"track\"].map(lambda t: dataset.mapping()[2].get(t))\n",
    "TRACK_META = TRACK_META[~np.isnan(TRACK_META[\"track_index\"])]\n",
    "TRACK_META[\"track_index\"] = TRACK_META[\"track_index\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRACK_META[[\"artist\", \"album\", \"title\", \"track\", \"track_index\"]].to_csv(\"track_meta.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRACK_META = pd.read_csv(\"track_meta.csv\")\n",
    "TRACK_META = TRACK_META.set_index(\"track_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Делаем маппинги айди юзера -> индекс юзера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "user_mapping_raw = dataset.mapping()[0]\n",
    "user_mapping = {int(k):int(v) for k, v in user_mapping_raw.items()}\n",
    "\n",
    "with open('user_mapping.pickle', 'wb') as f:\n",
    "    pickle.dump(user_mapping, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "user_mapping_raw = dataset.mapping()[0]\n",
    "\n",
    "with open('user_mapping.pickle', 'wb') as f:\n",
    "    pickle.dump({int(v):int(k) for k, v in user_mapping_raw.items()}, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('user_mapping_inverse.pickle', 'wb') as f:\n",
    "    pickle.dump({int(k):int(v) for k, v in user_mapping_raw.items()}, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_MAPPING = dict()\n",
    "\n",
    "with open('user_mapping.pickle', 'rb') as f:\n",
    "    USER_MAPPING = pickle.load(f)\n",
    "\n",
    "with open('user_mapping_inverse.pickle', 'rb') as f:\n",
    "    USER_MAPPING_INVERSE = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Этап 2. IPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IPS: Оценим $P(O = 1|user, track)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зафиксируем, что элементарное событие - это факт того что пользователь \"увидел\" трек. \n",
    "Иными словами - это любое не-пустое значение в матрице user-item. \n",
    "Существует два фундаментально отличающихся процесса, которые могут порождать данное событие:\n",
    "1) Пользователь сам выбрал первый трек для прослушивания, в данном случае $P(O=1|user, track) = P(O=1| X_{user}, X_{track}, X_{hidden})$, где\n",
    "* $X_{user}$ - это признаки пользователя,\n",
    "* $X_{track}$ - признаки трека,\n",
    "* $X_{hidden}$ - ненаблюдаемые признаки, например - пользователь случайно услышал трек на радио, либо ему этот трек порекомендовал друг.\n",
    "\n",
    "2) Пользователь слушал наши рекомендации. Например, для Sequential рекоммендера, вероятность может быть записана как $P(O=1|user, track) = P(O=1| user, track, t)$, где $t$ - позиция трека в выдаче. Сама вероятность может грубо задаваться рекуррентной формулой $P(O=1| user, track, t) = P(O=1| X_u, X_{track}) * P(O=1| user, track, t-1)$.\n",
    "\n",
    "Как можно заметить, задача оценки $P(O=1|user,track)$, вообще говоря, не является тривиальной."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оценка на базе первого прослушивания пользователя"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем очень грубое допущение, и попробуем оценивать $P(O=1|user, track)$ только по первому процессу. Иными словами, построим модель $P(O=1|user, track) = P(O=1| X_{user}, X_{track}, X_{hidden})$.\n",
    "\n",
    "Чтобы не усложнять себе жизнь - проигнорируем признаки $X_{hidden}$, и получим модель $P(O=1| X_{user}, X_{track})$, которую можно попытаться оценить логистической регрессией, например,\n",
    "\n",
    "$P(O=1| X_{user}, X_{track}) = \\sigma (b_1 + b_2 * (e_u, e_i) + b_3 * b_u + b_4 * b_i)$.\n",
    "\n",
    "Величины $e_u, e_i, b_u, b_i$ мы возьмем из ранее обученной лайтфм."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовим датасет для обучения, в качестве true лейблов - возьмем прослушки инициированныме пользователем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Entry = namedtuple(\"session\", [\"user\", \"track\"])\n",
    "\n",
    "user_observations = []\n",
    "\n",
    "for k, user_data in data_stage_1.groupby(\"user\"):\n",
    "    first = None\n",
    "    for _, row in user_data.sort_values(\"timestamp\").iterrows():\n",
    "        if first is None:\n",
    "            first = row[\"track\"]\n",
    "            user_observations.append(Entry(row[\"user\"], first))\n",
    "\n",
    "        if row[\"message\"] == \"last\":\n",
    "            first = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_meta_inverse = TRACK_META.copy()\n",
    "track_meta_inverse['track_index'] = track_meta_inverse.index\n",
    "track_meta_inverse.set_index(\"track\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовим датасет, описательные признаки будут следующие,\n",
    "\n",
    "$[b_u, b_i, (e_u, e_i)]$\n",
    "\n",
    "Лейблы будут такие:\n",
    "* true - пользователь инициировал прослушку трека,\n",
    "* false - пользователь не инициировал прослушку трека."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_NEGATIVE_OBSERVATIONS = 20\n",
    "train_data = list()\n",
    "train_target = list()\n",
    "all_observed_tracks = track_meta_inverse.track_index\n",
    "\n",
    "for entry in user_observations:\n",
    "    if entry.track not in track_meta_inverse.index:\n",
    "        continue\n",
    "\n",
    "    if entry.user not in USER_MAPPING_INVERSE.keys():\n",
    "        continue\n",
    "\n",
    "    # собираем эмбедды\n",
    "    \n",
    "    u_ix = USER_MAPPING_INVERSE[entry.user]\n",
    "    oi_ix = track_meta_inverse.loc[entry.track]['track_index']\n",
    "    \n",
    "    e_u = USER_EMBEDDINGS[u_ix]\n",
    "    b_u = USER_BIASES[u_ix]\n",
    "    \n",
    "    e_oi = ITEM_EMBEDDINGS[oi_ix]\n",
    "    b_oi = ITEM_BIASES[oi_ix]\n",
    "\n",
    "    item_unobserved_ix = np.random.choice(all_observed_tracks, size=NUM_NEGATIVE_OBSERVATIONS)\n",
    "\n",
    "    e_ui = ITEM_EMBEDDINGS[item_unobserved_ix, :]\n",
    "    b_ui = ITEM_BIASES[item_unobserved_ix]\n",
    "\n",
    "    # считаем фичи\n",
    "\n",
    "    e_uoi_dot = e_u.dot(e_oi)\n",
    "    e_uui_dot = e_u.dot(e_ui.T)\n",
    "\n",
    "    train_data.append(np.concat([\n",
    "        np.repeat(b_u, 21)[np.newaxis, :], \n",
    "        np.append(b_oi, b_ui)[np.newaxis, :],\n",
    "        np.append(e_uoi_dot, e_uui_dot)[np.newaxis, :],\n",
    "    ], axis=0).T)\n",
    "\n",
    "    train_target.append([1] + [0] * NUM_NEGATIVE_OBSERVATIONS)\n",
    "\n",
    "train_data = np.concat(train_data, axis=0)\n",
    "train_target = np.concat(train_target, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(train_data, train_target)\n",
    "\n",
    "roc_auc_score(train_target, logreg.predict_proba(train_data)[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При значении ROC > 0.5 перфоманс уже должен быть лучше подхода без IPS\n",
    "\n",
    "> One might be worried that\n",
    "we need to perfectly reconstruct all propensities for effective learning. However, as we will show, **we merely need\n",
    "estimated propensities that are “better” than the naive assumption of observations being revealed uniformly**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Этап 3. Обучим SVD с IPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as td\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import typing\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# если код будет крашиться - можно снизить\n",
    "NUM_NEGATIVE_SAMPLES = 20\n",
    "\n",
    "positives = data_stage_2[data_stage_2[\"time\"] > 0.7].copy()\n",
    "\n",
    "user_counts = positives.groupby(\"user\").size()\n",
    "users = set(user_counts[user_counts >= 4].index.values)\n",
    "\n",
    "track_counts = positives.groupby(\"track\").size()\n",
    "tracks = set(track_counts[track_counts >= 4].index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplets = positives[positives[\"user\"].isin(users) & positives[\"track\"].isin(tracks)]\n",
    "triplets = triplets[[\"user\", \"track\"]].rename(columns={\"track\": \"track_pos\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplets =  pd.concat([triplets] * NUM_NEGATIVE_SAMPLES).sort_index().reset_index(drop=True)\n",
    "triplets[\"track_neg\"] = np.random.choice(range(50000+1), len(triplets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассчитаем $log P(O=1|user, track)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Если для определенного трека / пользователя у нас нет эмбедда, используем дефолтное значение\n",
    "DEFAULT_LOG_PROBA = float(np.log(1e-3))\n",
    "\n",
    "def compute_proba(user: int, item: int) -> float:\n",
    "    if user not in USER_MAPPING_INVERSE.keys():\n",
    "        return DEFAULT_LOG_PROBA\n",
    "\n",
    "    if item not in track_meta_inverse.index:\n",
    "        return DEFAULT_LOG_PROBA\n",
    "\n",
    "    user_ix = USER_MAPPING_INVERSE[user]\n",
    "    \n",
    "    u_e = USER_EMBEDDINGS[user_ix, :]\n",
    "    u_b = USER_BIASES[user_ix]\n",
    "\n",
    "    item_ix = track_meta_inverse.loc[item]['track_index']\n",
    "    \n",
    "    i_e = ITEM_EMBEDDINGS[item_ix, :]\n",
    "    i_b = ITEM_BIASES[item_ix]\n",
    "\n",
    "    ui_score = u_e.dot(i_e)\n",
    "    return float(logreg.predict_log_proba(\n",
    "        np.array([u_b, i_b, ui_score])[np.newaxis, :]\n",
    "    )[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplets[\"pos_proba\"] = triplets.progress_apply(lambda x: compute_proba(x['user'], x['track_pos']), axis=1)\n",
    "triplets[\"neg_proba\"] = triplets.progress_apply(lambda x: compute_proba(x['user'], x['track_neg']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplets.to_csv(\"./stage_2/dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplets = pd.read_csv(\"./stage_2/dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdm = np.random.random(len(triplets))\n",
    "train_data = triplets[rdm < 0.9]\n",
    "val_data = triplets[rdm >= 0.9]\n",
    "\n",
    "len(train_data), len(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVDData(pl.LightningDataModule):\n",
    "  def __init__(self, train_triplets, val_triplets):\n",
    "      super().__init__()\n",
    "      self.train_triplets = train_triplets\n",
    "      self.val_triplets = val_triplets\n",
    "\n",
    "  def _collect_data(self, triplets):\n",
    "      users = triplets[\"user\"].values\n",
    "      positives = triplets[\"track_pos\"].values\n",
    "      negatives = triplets[\"track_neg\"].values\n",
    "      pos_proba = triplets[\"pos_proba\"].values\n",
    "      neg_proba = triplets[\"neg_proba\"].values\n",
    "\n",
    "      return td.TensorDataset(\n",
    "            torch.from_numpy(users).long(),\n",
    "            torch.from_numpy(positives).long(),\n",
    "            torch.from_numpy(negatives).long(),\n",
    "            torch.from_numpy(pos_proba).float(),\n",
    "            torch.from_numpy(neg_proba).float(),\n",
    "      )\n",
    "\n",
    "  def prepare_data(self, stage=None):\n",
    "      if stage == \"fit\" or stage is None:\n",
    "        self.train_dataset = self._collect_data(self.train_triplets)\n",
    "        self.val_dataset = self._collect_data(self.val_triplets)\n",
    "      elif stage == \"test\" or stage is None:\n",
    "        self.test_dataset = self._collect_data(self.test_triplets)\n",
    "\n",
    "  def train_dataloader(self):\n",
    "      return td.DataLoader(self.train_dataset, batch_size=2048, shuffle=True, num_workers=0)\n",
    "\n",
    "  def val_dataloader(self):\n",
    "      return td.DataLoader(self.val_dataset, batch_size=2048, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVD будем оптимизировать через BPR loss с использованием и без использования IPS, иными словами\n",
    "1. Vanilla: $P((e_u, e_p) - (e_u, e_n))$\n",
    "2. IPS: $P((e_u, e_p) - (e_u, e_n)) / P(O_{u,p} = 1, O_{u,n} = 1)$.\n",
    "\n",
    "Считаем, что события $O_{u,p}$ и $O_{u,n}$ независимы, поэтому $P(O_{u,p} = 1, O_{u,n} = 1) = P(O_{u,p} = 1) P(O_{u,n} = 1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVD(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        user_size: int,\n",
    "        item_size: int,\n",
    "        embedding_dim: int = 100,\n",
    "        use_ips: bool = True,\n",
    "        lr: float = 1e-3,\n",
    "        weight_decay: float = 1e-6,\n",
    "        log_to_prog_bar: bool = True,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.log_to_prog_bar = log_to_prog_bar\n",
    "        self.use_ips = use_ips\n",
    "        \n",
    "        self.user_embeddings = nn.Embedding(user_size, embedding_dim)\n",
    "        \n",
    "        self.item_embeddings = nn.Embedding(item_size, embedding_dim)\n",
    "        self.item_bias = nn.Embedding(item_size, 1)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        user_id: torch.Tensor,\n",
    "        item_id: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        user_embedding = self.user_embeddings(user_id)\n",
    "        item_embedding = self.item_embeddings(item_id)\n",
    "        item_bias = self.item_bias(item_id)\n",
    "\n",
    "        # User bias использовать не будем, поскольку он сократится в формуле BPR\n",
    "        return (user_embedding * item_embedding).sum(axis=1) + item_bias\n",
    "\n",
    "    def _step(self, batch, batch_idx, metric, prog_bar=False):\n",
    "        user, pos, neg, pos_proba, neg_proba = batch\n",
    "        \n",
    "        pos_score = self(user, pos)\n",
    "        neg_score = self(user, neg)\n",
    "        \n",
    "        loss = F.logsigmoid(pos_score - neg_score)\n",
    "        if self.use_ips:\n",
    "            loss = -(loss - pos_proba - neg_proba).mean()\n",
    "        else:\n",
    "            loss = -loss.mean()\n",
    "        self.log(metric, loss, prog_bar=prog_bar)\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch: typing.Sequence[torch.Tensor], batch_idx: int) -> torch.Tensor:\n",
    "        return self._step(batch, batch_idx, \"train_loss\")\n",
    "\n",
    "    def validation_step(self, batch: typing.Sequence[torch.Tensor], batch_idx: int) -> torch.Tensor:\n",
    "        return self._step(batch, batch_idx, \"val_loss\", self.log_to_prog_bar)\n",
    "\n",
    "    def test_step(self, batch, batch_idx, prog_bar=False):\n",
    "        return self._step(batch, batch_idx, \"test_loss\", self.log_to_prog_bar)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, verbose=True)\n",
    "        scheduler = {\n",
    "            'scheduler': lr_scheduler,\n",
    "            'monitor': 'val_loss'\n",
    "        }\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_recommendations(svd: pl.LightningModule, users: np.array):\n",
    "    user_embeddings = svd.user_embeddings(torch.from_numpy(users))\n",
    "    item_embeddings = svd.item_embeddings(torch.from_numpy(np.arange(0, 50000, dtype=np.int32)))\n",
    "    item_biases = svd.item_bias(torch.from_numpy(np.arange(0, 50000, dtype=np.int32)))\n",
    "    scores = ((user_embeddings @ item_embeddings.T) + item_biases.T)\n",
    "    recs = scores.argsort(descending=True, axis=1)[:, :50].cpu().numpy()\n",
    "\n",
    "    result = dict()\n",
    "    for u, r in zip(users, recs):\n",
    "        result[int(u)] = r.tolist()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_recommendations(recommendations: dict, fhandle):\n",
    "    result = list()\n",
    "    for user, tracks in recommendations.items():\n",
    "        result.append(json.dumps({\"user\": user, \"tracks\": tracks}))\n",
    "        \n",
    "    fhandle.write(\"\\n\".join(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = SVDData(train_data, val_data)\n",
    "svd_ips = SVD(10001, 50001, embedding_dim=50, use_ips=True, lr=1e-2).float()\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=50,\n",
    "    accelerator='gpu',\n",
    "    devices=1,\n",
    "    callbacks=[\n",
    "        pl.callbacks.early_stopping.EarlyStopping(monitor=\"val_loss\", patience=5),\n",
    "        pl.callbacks.LearningRateMonitor(logging_interval=\"step\")\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.fit(svd_ips, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./stage_2/recommendations_svd_ips.json\", \"w\") as f:\n",
    "    recommendations = compute_recommendations(svd_ips, triplets['user'].unique())\n",
    "    save_recommendations(recommendations, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = SVDData(train_data, val_data)\n",
    "svd = SVD(10001, 50001, embedding_dim=50, use_ips=False, lr=1e-2).float()\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=30,\n",
    "    accelerator='gpu',\n",
    "    devices=1,\n",
    "    callbacks=[\n",
    "        pl.callbacks.early_stopping.EarlyStopping(monitor=\"val_loss\", patience=5),\n",
    "        pl.callbacks.LearningRateMonitor(logging_interval=\"step\")\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.fit(svd, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./stage_2/recommendations_svd.json\", \"w\") as f:\n",
    "    recommendations = compute_recommendations(svd, triplets['user'].unique())\n",
    "    save_recommendations(recommendations, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
