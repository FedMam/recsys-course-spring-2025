{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "\n",
    "import lightfm\n",
    "import lightfm.data as ld\n",
    "import lightfm.evaluation as lv\n",
    "\n",
    "import tqdm\n",
    "import json\n",
    "import glob\n",
    "import faiss\n",
    "#import optuna\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import matplotlib.pyplot as pl\n",
    "import seaborn as sns\n",
    "\n",
    "np.random.seed(31337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Данные к семинару: https://drive.google.com/drive/folders/128GhNeYd3DTDuORdB2Hqtcd9YtKu0qd-?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/500kfeedbacks.csv\", index_col=None).drop(\"Unnamed: 0\", axis=1)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "positives = data[data[\"time\"] > 0.8].copy()\n",
    "positives[\"test\"] = np.random.random(len(positives)) >= 0.7\n",
    "positives.drop_duplicates([\"user\", \"track\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_counts = positives[~positives[\"test\"]].groupby(\"user\").size()\n",
    "users = set(user_counts[user_counts >= 3].index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "track_counts = positives[~positives[\"test\"]].groupby(\"track\").size()\n",
    "tracks = set(track_counts[track_counts >= 3].index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(users), len(tracks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучим LightFM (тут ничего нового)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = positives[~positives[\"test\"] & positives[\"user\"].isin(users) & positives[\"track\"].isin(tracks)]\n",
    "test_data = positives[positives[\"test\"] & positives[\"user\"].isin(users) & positives[\"track\"].isin(tracks)]\n",
    "\n",
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = ld.Dataset()\n",
    "dataset.fit(users, tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_interactions, _ = dataset.build_interactions(train_data[[\"user\", \"track\"]].itertuples(index=False, name=None))\n",
    "test_interactions, _ = dataset.build_interactions(test_data[[\"user\", \"track\"]].itertuples(index=False, name=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fit_model(\n",
    "    epochs=1, \n",
    "    at=10,\n",
    "    loss=\"warp\",\n",
    "    no_components=30,\n",
    "    learning_rate=0.01, \n",
    "    max_sampled=10,\n",
    "    user_alpha=0.0, \n",
    "    item_alpha=0.0, \n",
    "    threads=30, \n",
    "    verbose=False,\n",
    "    patience=3,\n",
    "    epsilon=1e-6,\n",
    "):\n",
    "    model = lightfm.LightFM(\n",
    "        no_components=no_components,\n",
    "        loss=loss,\n",
    "        learning_rate=learning_rate,\n",
    "        max_sampled=max_sampled,\n",
    "        user_alpha=user_alpha,\n",
    "        item_alpha=item_alpha,\n",
    "    )\n",
    "\n",
    "    precisions_at = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model = model.fit_partial(train_interactions, num_threads=threads)\n",
    "        \n",
    "        precision_at = lv.precision_at_k(model, test_interactions, train_interactions=train_interactions, k=at, num_threads=threads)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"{epoch}:\\t{np.mean(precision_at)} +/- {ss.sem(precision_at) * 1.96}\")\n",
    "            \n",
    "        precisions_at.append(np.mean(precision_at))\n",
    "            \n",
    "        if epoch > patience and all([precisions_at[-j] - precisions_at[-patience-1] < epsilon for j in range(1, patience + 1)]):\n",
    "            if verbose:\n",
    "                print(\"Early stopiing!\")\n",
    "            break\n",
    "        \n",
    "    else:\n",
    "        if verbose:\n",
    "            print(\"No early stopiing happened: increase epochs maybe?\")\n",
    "        \n",
    "    return model, precisions_at\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    loss = trial.suggest_categorical(\"loss\", [\"warp\", \"bpr\"])\n",
    "    no_components = trial.suggest_categorical(\"no_components\", [10, 30, 50])\n",
    "    learning_rate = trial.suggest_categorical(\"learning_rate\", [0.0001, 0.001, 0.01])\n",
    "    max_sampled = trial.suggest_categorical(\"max_sampled\", [10, 20, 50, 100])\n",
    "    user_alpha = trial.suggest_categorical(\"user_alpha\", [0.0, 0.0001])\n",
    "    item_alpha = trial.suggest_categorical(\"item_alpha\", [0.0, 0.0001])\n",
    "    \n",
    "    model, precisions_at = fit_model(\n",
    "        epochs=5, \n",
    "        at=10,\n",
    "        loss=loss,\n",
    "        no_components=no_components, \n",
    "        learning_rate=learning_rate, \n",
    "        max_sampled=max_sampled, \n",
    "        user_alpha=user_alpha, \n",
    "        item_alpha=item_alpha,\n",
    "    )\n",
    "    \n",
    "    return precisions_at[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# study = optuna.create_study(direction=\"maximize\")\n",
    "# study.optimize(objective, n_trials=30)\n",
    "# best_params = study.best_params\n",
    "\n",
    "best_params = {\n",
    "    'loss': 'warp',\n",
    "    'no_components': 50,\n",
    "    'learning_rate': 0.01,\n",
    "    'max_sampled': 100,\n",
    "    'user_alpha': 0.0,\n",
    "    'item_alpha': 0.0001\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model, precisions_at = fit_model(\n",
    "    epochs=300,\n",
    "    at=10,\n",
    "    loss=best_params[\"loss\"],\n",
    "    no_components=best_params[\"no_components\"], \n",
    "    learning_rate=best_params[\"learning_rate\"], \n",
    "    max_sampled=best_params[\"max_sampled\"],\n",
    "    user_alpha=best_params[\"user_alpha\"],\n",
    "    item_alpha=best_params[\"item_alpha\"],\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, ax = pl.subplots()\n",
    "\n",
    "ax.plot(np.arange(len(precisions_at)), precisions_at)\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Делаем маппинги индекс трека <-> айди трека и сохраняем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRACK_META = pd.read_json(\"./data/tracks.json\", lines=True)\n",
    "TRACK_META[\"track_index\"] = TRACK_META[\"track\"].map(lambda t: dataset.mapping()[2].get(t))\n",
    "TRACK_META = TRACK_META[~np.isnan(TRACK_META[\"track_index\"])]\n",
    "TRACK_META[\"track_index\"] = TRACK_META[\"track_index\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRACK_META"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сохраняем эмбеддинги"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "item_biases, item_embeddings = model.get_item_representations()\n",
    "user_biases, user_embeddings = model.get_user_representations()\n",
    "\n",
    "np.save(\"item_biases\", item_biases)\n",
    "np.save(\"item_embeddings\", item_embeddings)\n",
    "np.save(\"user_biases\", user_biases)\n",
    "np.save(\"user_embeddings\", user_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITEM_BIASES = np.load(\"item_biases.npy\")\n",
    "ITEM_EMBEDDINGS = np.load(\"item_embeddings.npy\")\n",
    "USER_BIASES = np.load(\"user_biases.npy\")\n",
    "USER_EMBEDDINGS = np.load(\"user_embeddings.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Рассчитываем \"прямые\" рекомендации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITEM_EMBEDDINGS_WITH_BIASES = np.concat([ITEM_BIASES[:, np.newaxis], np.ones(len(ITEM_BIASES))[:, np.newaxis], ITEM_EMBEDDINGS], axis=1)\n",
    "USER_EMBEDDINGS_WITH_BIASES = np.concat([np.ones(len(USER_BIASES))[:, np.newaxis], USER_BIASES[:, np.newaxis], USER_EMBEDDINGS], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_RECOMMENDATIONS = USER_EMBEDDINGS_WITH_BIASES.dot(ITEM_EMBEDDINGS_WITH_BIASES.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_USER_RECOMMENDATIONS = USER_RECOMMENDATIONS.argsort(axis=-1)[:, :200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"top_recommendations_raw\", TOP_USER_RECOMMENDATIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_USER_RECOMMENDATIONS = np.load(\"top_recommendations_raw.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Делаем маппинги индекс трека -> айди трека"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRACK_META = pd.read_json(\"./data/tracks.json\", lines=True)\n",
    "TRACK_META[\"track_index\"] = TRACK_META[\"track\"].map(lambda t: dataset.mapping()[2].get(t))\n",
    "TRACK_META = TRACK_META[~np.isnan(TRACK_META[\"track_index\"])]\n",
    "TRACK_META[\"track_index\"] = TRACK_META[\"track_index\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRACK_META[[\"artist\", \"album\", \"title\", \"track\", \"track_index\"]].to_csv(\"track_meta.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRACK_META = pd.read_csv(\"track_meta.csv\")\n",
    "TRACK_META = TRACK_META.set_index(\"track_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Делаем маппинги айди юзера -> индекс юзера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "user_mapping_raw = dataset.mapping()[0]\n",
    "user_mapping = {v:int(k) for k, v in user_mapping_raw.items()}\n",
    "\n",
    "with open('user_mapping.pickle', 'wb') as f:\n",
    "    pickle.dump(user_mapping, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_MAPPING = dict()\n",
    "\n",
    "with open('user_mapping.pickle', 'rb') as f:\n",
    "    USER_MAPPING = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подход #1. Разнообразие с использованием DPP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Алгоритм рассчета матрицы $L$ с параметризацией через $\\alpha,\\sigma$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://dl.acm.org/doi/pdf/10.1145/3269206.3272018\n",
    "\n",
    "Для пользователя $u$ и списка рекомендаций $W$,\n",
    "\n",
    "Рассчитаем матрицу $L$,\n",
    "1. Берем вектор пользователя $q_u$.\n",
    "2. Забираем эмбеддинги айтемов для списка рекоммендаций $v_i, i \\in W$.\n",
    "3. Рассчитываем диагональные элементы матрицы $L_{ii} = (q_u, v_i), i \\in W$.\n",
    "4. Рассчитываем off-diagonal элементы матрицы $L_{ij} = \\alpha (q_u, v_i)(q_u, v_j) exp(-\\frac{(v_i, v_j)}{2\\sigma^2}); i \\neq j; i,j \\in W$.\n",
    "5. Получаем матрицу $L$.\n",
    "6. Если матрица $L$ не является неотрицательно-определенной, то выполняем хак.\n",
    "\n",
    "Хак,\n",
    "1. Делаем разложение матрицы L на собственные числа (получаем собственные числа и собственные вектора ($\\lambda, e$).\n",
    "2. Зануляем отрицательные $\\lambda$.\n",
    "3. Восстанавливаем матрицу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 1.55\n",
    "SIGMA = 0.9\n",
    "\n",
    "# Расстояние между айдемами, в формуле D\n",
    "ITEM_DISTANCES = 1 - cosine_similarity(ITEM_EMBEDDINGS) + 1e-6\n",
    "# Выражение exp(-(vi, vj) / 2 sigma^2)\n",
    "RBF = np.exp(-ITEM_DISTANCES / (2 * (SIGMA ** 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_spd(m: np.array):\n",
    "    evalues, evecs = np.linalg.eig(m)\n",
    "    evalues[evalues < 0] = 1e-8\n",
    "    lamb = np.diag(evalues)\n",
    "    return evecs @ lamb @ np.linalg.inv(evecs)\n",
    "    \n",
    "def compute_kernel(user_index: int, item_indices: list[int]) -> np.array:\n",
    "    # В формуле (q, v)\n",
    "    scores = (ITEM_EMBEDDINGS_WITH_BIASES[item_indices, :].dot(USER_EMBEDDINGS_WITH_BIASES[user_index, :]))[np.newaxis, :]\n",
    "    # В формуле (q, vi)(q, vj)\n",
    "    scores_outer = scores.T.dot(scores)\n",
    "\n",
    "    # Рассчитываем матрицу L\n",
    "    diag_ix = np.diag_indices(len(item_indices))\n",
    "    rbf_minor = RBF[item_indices, :][:, item_indices]\n",
    "    kernel = ALPHA * scores_outer * rbf_minor\n",
    "    kernel[diag_ix] = scores_outer[diag_ix]\n",
    "    \n",
    "    return ensure_spd(kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выполняем переранжирование с помощью DPP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Входные параметры,\n",
    "* $u$ - индекс пользователя\n",
    "* $W$ - список рекомендаций (индексы треков)\n",
    "* $w$ - окно\n",
    "\n",
    "0. Инициализируем R (переранжированный список рекомендаций) пустым списком.\n",
    "1. Рассчитываем матрицу L с помощью алгоритма параграфом выше.\n",
    "2. С помощью жадого алгоритма, строим список из $w$ переранжированных айтемов $M$ для матрицы $L[W,W]$\n",
    "3. Добавляем эти айтемы в список $R$ и убираем их из списка $W$\n",
    "4. Повторяем пункты 2-3 до тех пор, пока $|W| > 0$\n",
    "\n",
    "Жадный алгоритм,\n",
    "\n",
    "1. Создаем список ранжированных айтемов $R_w$ для окна $w$\n",
    "2. Ищем такой элемент $i$, который при котором $det(L[R_w \\cup i, R_w \\cup i])$ максимальна.\n",
    "3. Добавляем этот элемент $i$ в список $R_w$.\n",
    "4. Итерируемся $w$ раз\n",
    "5. Возвращаем получившийся список индексов $R_w$\n",
    "\n",
    "https://dl.acm.org/doi/pdf/10.1145/3269206.3272018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_approx_max(kernel: np.array, size: int) -> list[int]:\n",
    "    assert kernel.shape[0] == kernel.shape[1], \"invalid kernel size\"\n",
    "    items = [np.argmax(np.diag(kernel))]\n",
    "\n",
    "    for i in range(size - 1):\n",
    "        max_c = None\n",
    "        max_v = None\n",
    "        for c in range(kernel.shape[0]):\n",
    "            if c in items:\n",
    "                continue\n",
    "            items_c = items + [c]\n",
    "            cur_v = np.linalg.det(kernel[items_c, :][:, items_c])\n",
    "            if max_v is None or max_v < cur_v:\n",
    "                max_v = cur_v\n",
    "                max_c = c\n",
    "        items = items + [max_c]\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dpp_rerank(user_index: int, item_indices: list[int], window: int):\n",
    "    L = compute_kernel(user_index, item_indices)\n",
    "    m = dict(zip(range(len(item_indices)), item_indices))\n",
    "\n",
    "    R = list()\n",
    "    W = np.arange(len(item_indices), dtype=np.int32)\n",
    "    \n",
    "    while len(W) != 0:\n",
    "        M = greedy_approx_max(L[W,:][:,W], min(len(W), window))\n",
    "        D = list()\n",
    "        for ix in M:\n",
    "            R.append(W[ix])\n",
    "            D.append(W[ix])\n",
    "        W = W[~np.isin(W, D)]\n",
    "    \n",
    "    return [int(m[r]) for r in R]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Посмотрим, а оно вообще работает?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_USER = 1\n",
    "raw_recommendations = TOP_USER_RECOMMENDATIONS[test_user]\n",
    "dpp_recommendations = dpp_rerank(test_user, raw_recommendations, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRACK_META.loc[raw_recommendations][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRACK_META.loc[dpp_recommendations][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Переранжируем рекомендации с использованием DPP\n",
    "\n",
    "При переранжировании возьмем 50 топовых (вряд ли пользователь потребит все)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_USER_RECOMMENDATIONS_DIVERSIFIED = list()\n",
    "for u, recs in tqdm.tqdm(enumerate(TOP_USER_RECOMMENDATIONS)):\n",
    "    TOP_USER_RECOMMENDATIONS_DIVERSIFIED.append(dpp_rerank(u, recs[:50], 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_USER_RECOMMENDATIONS_DIVERSIFIED = np.array(TOP_USER_RECOMMENDATIONS_DIVERSIFIED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сохраним рекомендации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_recs(recommendations: np.array, fhandle):\n",
    "    result = list()\n",
    "    for user_ix, tracks_ixs in enumerate(recommendations):\n",
    "        user_id = USER_MAPPING[user_ix]\n",
    "        tracks_ids = TRACK_META.loc[tracks_ixs][\"track\"].values.astype(int).tolist()\n",
    "\n",
    "        result.append(json.dumps({\n",
    "            \"user\": user_id,\n",
    "            \"tracks\": tracks_ids\n",
    "        }))\n",
    "        \n",
    "    fhandle.write(\"\\n\".join(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./result/recommendations_lfm.json\", \"w\") as fh:\n",
    "    save_recs(TOP_USER_RECOMMENDATIONS, fh)\n",
    "\n",
    "with open(\"./result/recommendations_lfm_dpp.json\", \"w\") as fh:\n",
    "    save_recs(TOP_USER_RECOMMENDATIONS_DIVERSIFIED, fh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подход #2. Разнообразие с использованием эвристики"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве простого альтернативного подхода - попробуем ограничить количество треков от одного артиста.\n",
    "Идея здесь такая - чем больше уникальных артистов в нашей выдаче, тем разнообразнее наши рекомендации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 50\n",
    "MAX_TRACKS_FROM_SAME_ARTIST = 2\n",
    "\n",
    "ARTIST_DIVERSIFIED_RECOMMENDATIONS = list()\n",
    "for user_index, recs_indices in tqdm.tqdm(enumerate(TOP_USER_RECOMMENDATIONS)):\n",
    "    artists = dict()\n",
    "    recs_ids = list()\n",
    "    for rec_ix in recs_indices:\n",
    "        if len(recs_ids) >= K:\n",
    "            break\n",
    "        \n",
    "        track = TRACK_META.loc[rec_ix]\n",
    "        artist = track['artist']\n",
    "\n",
    "        if artist not in artists.keys():\n",
    "            artists[artist] = 0\n",
    "        artists[artist] += 1\n",
    "\n",
    "        if artists[artist] <= MAX_TRACKS_FROM_SAME_ARTIST:\n",
    "            recs_ids.append(int(track['track']))\n",
    "\n",
    "    ARTIST_DIVERSIFIED_RECOMMENDATIONS.append({\n",
    "        \"user\": USER_MAPPING[user_index],\n",
    "        \"tracks\": recs_ids\n",
    "    })\n",
    "\n",
    "\n",
    "with open(\"./result/recommendations_lfm_auth.json\", \"w\") as fh:\n",
    "   for line in ARTIST_DIVERSIFIED_RECOMMENDATIONS:\n",
    "       fh.write(json.dumps(line) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRACK_META.set_index(\"track\").loc[ARTIST_DIVERSIFIED_RECOMMENDATIONS[TEST_USER][\"tracks\"]][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
